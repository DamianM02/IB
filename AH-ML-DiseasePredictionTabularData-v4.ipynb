{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "392997cb-ad98-4273-926b-37d4a027f4f6",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr>    \n",
    "<td style=\"text-align: center\">\n",
    "<h1>Machine Learning for Disease Prediction - Working with Tabular Data</h1>\n",
    "<h2><a href=\"http://home.agh.edu.pl/~horzyk/index.php\">Adrian Horzyk</a></h2>\n",
    "</td> \n",
    "<td>\n",
    "<img src=\"http://home.agh.edu.pl/~horzyk/im/AdrianHorzyk49BT140h.png\" alt=\"Adrian Horzyk, Professor\" title=\"Adrian Horzyk, Professor\" />        \n",
    "</td> \n",
    "</tr>\n",
    "</table>\n",
    "<h3><i>Welcome to the interactive notebook where you can learn how neural networks work, experience and check their operation on selected data sets, and conduct your own experiments.</i></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6c97b41-9bc9-4d23-b689-51b3f212ccf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/biomedicine/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import subprocess\n",
    " \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8c33f63-cf4b-46d8-aea0-834a65b474b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load different biomedical datasets\n",
    "def load_dataset(dataset_name):\n",
    "    datasets = {\n",
    "        \"diabetes\": {\n",
    "            \"url\": \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\",\n",
    "            \"columns\": ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome']\n",
    "        },\n",
    "        \"heart_disease\": {\n",
    "            \"url\": \"https://raw.githubusercontent.com/ageron/handson-ml2/master/datasets/heart/heart.csv\",\n",
    "            \"columns\": None  # Column names are included in the dataset\n",
    "        },\n",
    "        \"breast_cancer\": {\n",
    "            \"url\": \"https://raw.githubusercontent.com/dataprofessor/data/master/breast_cancer.csv\",\n",
    "            \"columns\": None  # Column names are included in the dataset\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    if dataset_name in datasets:\n",
    "        url = datasets[dataset_name][\"url\"]\n",
    "        columns = datasets[dataset_name][\"columns\"]\n",
    "        df = pd.read_csv(url, names=columns) if columns else pd.read_csv(url)\n",
    "        return df\n",
    "    else:\n",
    "        raise ValueError(\"Dataset not found. Available options: 'diabetes', 'heart_disease', 'breast_cancer'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fc191a-2dc3-42ab-a7e4-0c27db1412b5",
   "metadata": {},
   "source": [
    "## Here, we choose \"diabetes\" dataset out of the above three for the following experiments.\n",
    "### After making yourself familiar with this notebook, use also the other two datasets for similar experiments for training your skills and understanding of this topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f25befec-f842-4ac0-956d-2dd1c36e27bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "selected_dataset = \"diabetes\"  # Change this to 'heart_disease' or 'breast_cancer' to try other datasets\n",
    "df = load_dataset(selected_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f9b23b2-a06b-43f8-820f-b842d07ee8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values using mean imputation for numerical columns\n",
    "imputer = SimpleImputer(strategy='mean')  # Can change to 'median' or 'most_frequent'\n",
    "df.iloc[:, :-1] = imputer.fit_transform(df.iloc[:, :-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "661ec34d-3813-45d4-950a-b75c9d0d0e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split (80% training, 20% testing)\n",
    "X = df.drop(columns=[df.columns[-1]])  # Last column is assumed to be the target\n",
    "y = df[df.columns[-1]]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10e99ac3-1504-4941-9ed5-1223933ec447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling using StandardScaler for better performance in distance-based models\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82116c8-4014-4458-8ea4-8df378763bf1",
   "metadata": {},
   "source": [
    "## Exercise: Experiment with different hyperparameters\n",
    "Try modifying n_estimators, max_depth, learning_rate, kernel types, and other settings to see how performance changes.\n",
    "Conduct some experiments with the hyperparameters and try to achieve better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0878c1-d7c6-48cb-b9ec-94c44716a735",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mJądro Kernel uległo awarii podczas wykonywania kodu w bieżącej komórce lub w poprzedniej komórce. \n",
      "\u001b[1;31mPrzejrzyj kod w komórkach, aby zidentyfikować możliwą przyczynę awarii. \n",
      "\u001b[1;31mKliknij <a href='https://aka.ms/vscodeJupyterKernelCrash'>tutaj</a>, aby uzyskać więcej informacji. \n",
      "\u001b[1;31mAby uzyskać dalsze szczegóły, wyświetl <a href='command:jupyter.viewOutput'>dziennik</a> Jupyter."
     ]
    }
   ],
   "source": [
    "# Dictionary of machine learning models with key hyperparameters\n",
    "models = {\n",
    "    \"RandomForest\": RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=5,\n",
    "        random_state=42\n",
    "    ),\n",
    "    \"SVM\": SVC(\n",
    "        kernel='rbf',\n",
    "        C=1,\n",
    "        gamma='scale',\n",
    "        probability=True\n",
    "    ),\n",
    "    \"XGBoost\": XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=5,\n",
    "        scale_pos_weight=10\n",
    "    ),\n",
    "    \"ANN\": Sequential([\n",
    "        Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ]),\n",
    "    \"DNN\": Sequential([\n",
    "        Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "        Dropout(0.3),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f27432-4a7f-4d4e-82f0-66b436f59bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile and fit ANN & DNN models\n",
    "for model_name in [\"ANN\", \"DNN\"]:\n",
    "    models[model_name].compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    models[model_name].fit(X_train, y_train, epochs=50, batch_size=32,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb19ef9-8cf0-4e28-a7fd-c0a8d3930f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and Evaluation of ML models\n",
    "results = {}\n",
    "y_prob_dict = {}\n",
    "for name, model in models.items():\n",
    "    if name in [\"ANN\", \"DNN\"]:\n",
    "        y_prob = model.predict(X_test).flatten()  # Keep probabilities for AUC-ROC\n",
    "        y_pred = (y_prob > 0.5).astype(int)  # Convert probabilities to binary labels\n",
    "    else:\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_prob = model.predict_proba(X_test)[:,1] if hasattr(model, 'predict_proba') else None\n",
    "    \n",
    "    y_prob_dict[name] = y_prob\n",
    "    results[name] = {\n",
    "        \"Accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"Precision\": precision_score(y_test, y_pred),\n",
    "        \"Recall\": recall_score(y_test, y_pred),\n",
    "        \"F1 Score\": f1_score(y_test, y_pred),\n",
    "        \"ROC-AUC\": roc_auc_score(y_test, y_prob) if (y_prob is not None and len(np.unique(y_test)) > 1) else None\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b03d33-b0da-4e9a-acb8-1cc8804f6eef",
   "metadata": {},
   "source": [
    "## Exercise: Modify the ANN structure\n",
    "Try changing the number of layers, neurons, activation functions, or optimizer settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a29332-7788-4dc1-8368-a842489eb387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network (ANN) with three layers\n",
    "ann = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),  # First hidden layer with 64 neurons\n",
    "    Dense(32, activation='relu'),  # Second hidden layer with 32 neurons\n",
    "    Dense(1, activation='sigmoid')  # Output layer for binary classification\n",
    "])\n",
    "ann.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "ann.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test), verbose=0)  # 50 epochs, batch size 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f35711b-970f-4ccc-9a44-60f5e8be4491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions for ANN\n",
    "y_pred_ann = (ann.predict(X_test) > 0.5).astype(int).flatten()\n",
    "y_prob_ann = ann.predict(X_test)\n",
    "results[\"ANN\"] = {\n",
    "    \"Accuracy\": accuracy_score(y_test, y_pred_ann),\n",
    "    \"Precision\": precision_score(y_test, y_pred_ann),\n",
    "    \"Recall\": recall_score(y_test, y_pred_ann),\n",
    "    \"F1 Score\": f1_score(y_test, y_pred_ann),\n",
    "    \"ROC-AUC\": roc_auc_score(y_test, y_prob_ann)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41ee92c-167f-41ed-91d8-26f4cea210d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer-Based Model (TabNet) without the verbose parameter\n",
    "tabnet = TabNetClassifier()\n",
    "tabnet.fit(X_train, y_train, eval_set=[(X_test, y_test)], max_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac04375-48c2-45d8-835c-41aadf90f92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions for TabNet\n",
    "y_pred_tabnet = tabnet.predict(X_test)\n",
    "y_prob_tabnet = tabnet.predict_proba(X_test)[:,1]\n",
    "results[\"TabNet\"] = {\n",
    "    \"Accuracy\": accuracy_score(y_test, y_pred_tabnet),\n",
    "    \"Precision\": precision_score(y_test, y_pred_tabnet),\n",
    "    \"Recall\": recall_score(y_test, y_pred_tabnet),\n",
    "    \"F1 Score\": f1_score(y_test, y_pred_tabnet),\n",
    "    \"ROC-AUC\": roc_auc_score(y_test, y_prob_tabnet)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bede668-f43d-4433-a998-7c1dc61bf308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curves\n",
    "plt.figure(figsize=(8, 6))\n",
    "plotted = False  # To check if at least one valid plot exists\n",
    "\n",
    "for name, y_prob in y_prob_dict.items():\n",
    "    if y_prob is not None and len(y_prob) > 0:\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "        plt.plot(fpr, tpr, label=f\"{name} (AUC = {roc_auc_score(y_test, y_prob):.2f})\")\n",
    "        plotted = True\n",
    "\n",
    "if plotted:\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No valid probability predictions found. Ensure models support `predict_proba()`.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b537582-771a-43ae-8b8e-65c6432a8466",
   "metadata": {},
   "source": [
    "### In the above chart, the Random Forest has the best ROC Curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c308325e-e047-4ddd-a4de-5b2d897ccf66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display feature importance for XGBoost\n",
    "xgb_model = XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=5)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.barh(X.columns, xgb_model.feature_importances_)\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.ylabel('Feature Name')\n",
    "plt.title('XGBoost Feature Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87184a7-3544-499b-8052-74e941f9e764",
   "metadata": {},
   "source": [
    "### Here, we can calculate the impact of features on the XGBoost model performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3d250f-b0cb-4003-9ea4-8405d79b13a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Explainability\n",
    "explainer = shap.Explainer(XGBClassifier().fit(X_train, y_train))\n",
    "shap_values = explainer(X_test)\n",
    "shap.summary_plot(shap_values, X_test)  # Feature importance visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0198d52b-600d-4e07-a0c6-47b565db525c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results as a DataFrame\n",
    "results_df = pd.DataFrame(results).T\n",
    "print(\"Model Performance Comparison:\\n\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3758e86c-935c-4121-bdcd-d758e84c927e",
   "metadata": {},
   "source": [
    "### Usually, XGBoost and TabNet achieve the highest performances; however, here, they achieved weak results in comparison to e.g. RandomForest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0102c716-c0ff-442c-a924-64bfecf25a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If using Jupyter Notebook, display it as a table\n",
    "import IPython.display as display\n",
    "display.display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841b76d3-fe69-48dc-93da-21de004c10be",
   "metadata": {},
   "source": [
    "### As visible, no method is perfect for these datasets. Try to change some hyperparameters and experiment to collect better results.\n",
    "\n",
    "Try to change some XGBoost parameters and analyze how it influences the performance of this model on a chosen dataset in comparison to previous results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f7b774-6d21-4a02-83af-a22f4f740d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: Experiment with XGBoost Hyperparameters\n",
    "def tune_xgboost():\n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 100, 200],  # Experiment with the number of boosting rounds\n",
    "        'learning_rate': [0.01, 0.1, 0.2],  # Adjust the step size\n",
    "        'max_depth': [3, 5, 7],  # Increase model complexity\n",
    "        'subsample': [0.8, 1.0],  # Reduce overfitting by sampling data\n",
    "        'colsample_bytree': [0.8, 1.0]  # Feature selection at each boosting step\n",
    "    }\n",
    "    grid_search = GridSearchCV(XGBClassifier(), param_grid, cv=5, scoring='roc_auc', verbose=1, n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    print(\"Best XGBoost Hyperparameters:\", grid_search.best_params_)\n",
    "    return grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ce9289-574b-4a17-8e02-31c319c417e2",
   "metadata": {},
   "source": [
    "I have added an exercise for students to experiment with XGBoost hyperparameters using GridSearchCV. The exercise allows students to test different values for:\n",
    "<ul>\n",
    "<li>n_estimators</li>\n",
    "<li>learning_rate</li>\n",
    "<li>max_depth</li>\n",
    "<li>subsample</li>\n",
    "<li>colsample_bytree</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7d5842-06d8-42af-bd5a-f65461134c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, you can uncomment the following line to experiment with hyperparameter tuning using GridSearchCV:\n",
    "# best_xgb = tune_xgboost()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfecdf89-101e-41ea-9de4-f38c33644c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results as a DataFrame\n",
    "results_df = pd.DataFrame(results).T\n",
    "print(\"Model Performance Comparison:\")\n",
    "print(results_df)\n",
    "\n",
    "# Display results in Jupyter Notebook\n",
    "import IPython.display as display\n",
    "display.display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a80186f-4e15-4410-af38-d532929f1902",
   "metadata": {},
   "source": [
    "## Exercises to better understand and work with biomedical data\n",
    "1. Experiment with hyperparameters (e.g., n_estimators, max_depth, learning_rate, kernel types).\n",
    "2. Modify the ANN structure (changing the number of layers, neurons, activation functions, or optimizer settings).\n",
    "3. Try different biomedical datasets (e.g., Heart Disease, Breast Cancer, Stroke Prediction from Kaggle/UCI ML Repository) - some of them are available in the above code - prepared for your experiments.\n",
    "4. Compare and interpret results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f60d14-86a2-4c7a-85a8-870b3929aab4",
   "metadata": {},
   "source": [
    "## Final notice\n",
    "\n",
    "Use the gained experience to gradually construct your solution on a different dataset to complete the assignment with tabular data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d9e580-67dd-4b03-a6fc-63d52a928b47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "biomedicine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
